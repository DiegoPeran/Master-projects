{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PREDICTING WIND ENERGY PRODUCTION WITH SCIKIT-LEARN\n",
    "\n",
    "**Authors: David de la Fuente López y Diego Perán Vacas**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we have to do is to read the data, which is a Pandas dataframe in pickle format. For this purpose, we import pandas and run the corresponding function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_pickle('wind_pickle.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check the form of the Pandas dataframe. Since variables steps, month, day and hour cannot be used for training the models, we have to remove them. In order to do so, we are using two different procedures, to work with the techniques to  modify pandas dataframes. Firstly, we are creating a function to remove an specific column of the dataframe. Then we apply that function for the specific exercise, taking into account the variables we have to remove are in original places 2,4,5 and 6. Secondly, we are using a Pandas method called .drop(), including the argument axis = 1; to remove the columns we cannot use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>energy</th>\n",
       "      <th>steps</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>p54.162.1</th>\n",
       "      <th>p54.162.2</th>\n",
       "      <th>p54.162.3</th>\n",
       "      <th>p54.162.4</th>\n",
       "      <th>...</th>\n",
       "      <th>v100.16</th>\n",
       "      <th>v100.17</th>\n",
       "      <th>v100.18</th>\n",
       "      <th>v100.19</th>\n",
       "      <th>v100.20</th>\n",
       "      <th>v100.21</th>\n",
       "      <th>v100.22</th>\n",
       "      <th>v100.23</th>\n",
       "      <th>v100.24</th>\n",
       "      <th>v100.25</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>402.71</td>\n",
       "      <td>0</td>\n",
       "      <td>2005</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "      <td>2.534970e+06</td>\n",
       "      <td>2.526864e+06</td>\n",
       "      <td>2.518754e+06</td>\n",
       "      <td>2.510648e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.683596</td>\n",
       "      <td>-4.545396</td>\n",
       "      <td>-4.407196</td>\n",
       "      <td>-4.268996</td>\n",
       "      <td>-4.131295</td>\n",
       "      <td>-4.669626</td>\n",
       "      <td>-4.528932</td>\n",
       "      <td>-4.388736</td>\n",
       "      <td>-4.248540</td>\n",
       "      <td>-4.107846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>696.80</td>\n",
       "      <td>6</td>\n",
       "      <td>2005</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2.537369e+06</td>\n",
       "      <td>2.529277e+06</td>\n",
       "      <td>2.521184e+06</td>\n",
       "      <td>2.513088e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.397886</td>\n",
       "      <td>-3.257192</td>\n",
       "      <td>-3.115998</td>\n",
       "      <td>-2.975304</td>\n",
       "      <td>-2.834609</td>\n",
       "      <td>-3.396390</td>\n",
       "      <td>-3.254198</td>\n",
       "      <td>-3.112506</td>\n",
       "      <td>-2.970314</td>\n",
       "      <td>-2.828622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1591.15</td>\n",
       "      <td>12</td>\n",
       "      <td>2005</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>2.533727e+06</td>\n",
       "      <td>2.525703e+06</td>\n",
       "      <td>2.517678e+06</td>\n",
       "      <td>2.509654e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.454105</td>\n",
       "      <td>-1.296447</td>\n",
       "      <td>-1.138290</td>\n",
       "      <td>-0.980134</td>\n",
       "      <td>-0.822476</td>\n",
       "      <td>-1.459094</td>\n",
       "      <td>-1.302933</td>\n",
       "      <td>-1.147271</td>\n",
       "      <td>-0.991110</td>\n",
       "      <td>-0.834949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1338.62</td>\n",
       "      <td>18</td>\n",
       "      <td>2005</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>2.534491e+06</td>\n",
       "      <td>2.526548e+06</td>\n",
       "      <td>2.518609e+06</td>\n",
       "      <td>2.510670e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>1.255015</td>\n",
       "      <td>1.370265</td>\n",
       "      <td>1.485515</td>\n",
       "      <td>1.600765</td>\n",
       "      <td>1.716015</td>\n",
       "      <td>1.210612</td>\n",
       "      <td>1.319376</td>\n",
       "      <td>1.428140</td>\n",
       "      <td>1.536405</td>\n",
       "      <td>1.645169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>562.50</td>\n",
       "      <td>0</td>\n",
       "      <td>2005</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "      <td>2.529543e+06</td>\n",
       "      <td>2.521623e+06</td>\n",
       "      <td>2.513702e+06</td>\n",
       "      <td>2.505782e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>1.939031</td>\n",
       "      <td>2.023847</td>\n",
       "      <td>2.108663</td>\n",
       "      <td>2.193977</td>\n",
       "      <td>2.278793</td>\n",
       "      <td>1.873673</td>\n",
       "      <td>1.953000</td>\n",
       "      <td>2.031829</td>\n",
       "      <td>2.111157</td>\n",
       "      <td>2.189986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5932</th>\n",
       "      <td>211.78</td>\n",
       "      <td>0</td>\n",
       "      <td>2010</td>\n",
       "      <td>12</td>\n",
       "      <td>30</td>\n",
       "      <td>18</td>\n",
       "      <td>2.450279e+06</td>\n",
       "      <td>2.442801e+06</td>\n",
       "      <td>2.435324e+06</td>\n",
       "      <td>2.427846e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>3.473201</td>\n",
       "      <td>3.445761</td>\n",
       "      <td>3.418819</td>\n",
       "      <td>3.391379</td>\n",
       "      <td>3.363938</td>\n",
       "      <td>3.499644</td>\n",
       "      <td>3.467214</td>\n",
       "      <td>3.434785</td>\n",
       "      <td>3.401856</td>\n",
       "      <td>3.369426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5933</th>\n",
       "      <td>944.52</td>\n",
       "      <td>6</td>\n",
       "      <td>2010</td>\n",
       "      <td>12</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>2.455407e+06</td>\n",
       "      <td>2.447817e+06</td>\n",
       "      <td>2.440226e+06</td>\n",
       "      <td>2.432635e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>2.280789</td>\n",
       "      <td>2.372091</td>\n",
       "      <td>2.463892</td>\n",
       "      <td>2.555194</td>\n",
       "      <td>2.646994</td>\n",
       "      <td>2.333674</td>\n",
       "      <td>2.418490</td>\n",
       "      <td>2.503306</td>\n",
       "      <td>2.588621</td>\n",
       "      <td>2.673437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5934</th>\n",
       "      <td>224.06</td>\n",
       "      <td>12</td>\n",
       "      <td>2010</td>\n",
       "      <td>12</td>\n",
       "      <td>31</td>\n",
       "      <td>6</td>\n",
       "      <td>2.457296e+06</td>\n",
       "      <td>2.449624e+06</td>\n",
       "      <td>2.441947e+06</td>\n",
       "      <td>2.434271e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>2.211939</td>\n",
       "      <td>2.341657</td>\n",
       "      <td>2.470877</td>\n",
       "      <td>2.600096</td>\n",
       "      <td>2.729815</td>\n",
       "      <td>2.188489</td>\n",
       "      <td>2.312221</td>\n",
       "      <td>2.436451</td>\n",
       "      <td>2.560183</td>\n",
       "      <td>2.684413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5935</th>\n",
       "      <td>0.37</td>\n",
       "      <td>18</td>\n",
       "      <td>2010</td>\n",
       "      <td>12</td>\n",
       "      <td>31</td>\n",
       "      <td>12</td>\n",
       "      <td>2.464015e+06</td>\n",
       "      <td>2.456257e+06</td>\n",
       "      <td>2.448494e+06</td>\n",
       "      <td>2.440732e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>1.235059</td>\n",
       "      <td>1.311393</td>\n",
       "      <td>1.387228</td>\n",
       "      <td>1.463563</td>\n",
       "      <td>1.539897</td>\n",
       "      <td>1.263996</td>\n",
       "      <td>1.338335</td>\n",
       "      <td>1.412174</td>\n",
       "      <td>1.486513</td>\n",
       "      <td>1.560852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5936</th>\n",
       "      <td>70.14</td>\n",
       "      <td>0</td>\n",
       "      <td>2010</td>\n",
       "      <td>12</td>\n",
       "      <td>31</td>\n",
       "      <td>18</td>\n",
       "      <td>2.468054e+06</td>\n",
       "      <td>2.460197e+06</td>\n",
       "      <td>2.452335e+06</td>\n",
       "      <td>2.444478e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.887335</td>\n",
       "      <td>-0.762107</td>\n",
       "      <td>-0.636879</td>\n",
       "      <td>-0.512149</td>\n",
       "      <td>-0.386921</td>\n",
       "      <td>-0.840437</td>\n",
       "      <td>-0.716705</td>\n",
       "      <td>-0.592974</td>\n",
       "      <td>-0.469242</td>\n",
       "      <td>-0.345511</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5937 rows × 556 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       energy  steps  year  month  day  hour     p54.162.1     p54.162.2  \\\n",
       "0      402.71      0  2005      1    2    18  2.534970e+06  2.526864e+06   \n",
       "1      696.80      6  2005      1    3     0  2.537369e+06  2.529277e+06   \n",
       "2     1591.15     12  2005      1    3     6  2.533727e+06  2.525703e+06   \n",
       "3     1338.62     18  2005      1    3    12  2.534491e+06  2.526548e+06   \n",
       "4      562.50      0  2005      1    3    18  2.529543e+06  2.521623e+06   \n",
       "...       ...    ...   ...    ...  ...   ...           ...           ...   \n",
       "5932   211.78      0  2010     12   30    18  2.450279e+06  2.442801e+06   \n",
       "5933   944.52      6  2010     12   31     0  2.455407e+06  2.447817e+06   \n",
       "5934   224.06     12  2010     12   31     6  2.457296e+06  2.449624e+06   \n",
       "5935     0.37     18  2010     12   31    12  2.464015e+06  2.456257e+06   \n",
       "5936    70.14      0  2010     12   31    18  2.468054e+06  2.460197e+06   \n",
       "\n",
       "         p54.162.3     p54.162.4  ...   v100.16   v100.17   v100.18   v100.19  \\\n",
       "0     2.518754e+06  2.510648e+06  ... -4.683596 -4.545396 -4.407196 -4.268996   \n",
       "1     2.521184e+06  2.513088e+06  ... -3.397886 -3.257192 -3.115998 -2.975304   \n",
       "2     2.517678e+06  2.509654e+06  ... -1.454105 -1.296447 -1.138290 -0.980134   \n",
       "3     2.518609e+06  2.510670e+06  ...  1.255015  1.370265  1.485515  1.600765   \n",
       "4     2.513702e+06  2.505782e+06  ...  1.939031  2.023847  2.108663  2.193977   \n",
       "...            ...           ...  ...       ...       ...       ...       ...   \n",
       "5932  2.435324e+06  2.427846e+06  ...  3.473201  3.445761  3.418819  3.391379   \n",
       "5933  2.440226e+06  2.432635e+06  ...  2.280789  2.372091  2.463892  2.555194   \n",
       "5934  2.441947e+06  2.434271e+06  ...  2.211939  2.341657  2.470877  2.600096   \n",
       "5935  2.448494e+06  2.440732e+06  ...  1.235059  1.311393  1.387228  1.463563   \n",
       "5936  2.452335e+06  2.444478e+06  ... -0.887335 -0.762107 -0.636879 -0.512149   \n",
       "\n",
       "       v100.20   v100.21   v100.22   v100.23   v100.24   v100.25  \n",
       "0    -4.131295 -4.669626 -4.528932 -4.388736 -4.248540 -4.107846  \n",
       "1    -2.834609 -3.396390 -3.254198 -3.112506 -2.970314 -2.828622  \n",
       "2    -0.822476 -1.459094 -1.302933 -1.147271 -0.991110 -0.834949  \n",
       "3     1.716015  1.210612  1.319376  1.428140  1.536405  1.645169  \n",
       "4     2.278793  1.873673  1.953000  2.031829  2.111157  2.189986  \n",
       "...        ...       ...       ...       ...       ...       ...  \n",
       "5932  3.363938  3.499644  3.467214  3.434785  3.401856  3.369426  \n",
       "5933  2.646994  2.333674  2.418490  2.503306  2.588621  2.673437  \n",
       "5934  2.729815  2.188489  2.312221  2.436451  2.560183  2.684413  \n",
       "5935  1.539897  1.263996  1.338335  1.412174  1.486513  1.560852  \n",
       "5936 -0.386921 -0.840437 -0.716705 -0.592974 -0.469242 -0.345511  \n",
       "\n",
       "[5937 rows x 556 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_column(X,string):\n",
    "    return X.iloc[:, X.columns != string]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = drop_column(data,'steps')\n",
    "data = drop_column(data,'month')\n",
    "data = drop_column(data,'day')\n",
    "data = drop_column(data,'hour')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>energy</th>\n",
       "      <th>year</th>\n",
       "      <th>p54.162.1</th>\n",
       "      <th>p54.162.2</th>\n",
       "      <th>p54.162.3</th>\n",
       "      <th>p54.162.4</th>\n",
       "      <th>p54.162.5</th>\n",
       "      <th>p54.162.6</th>\n",
       "      <th>p54.162.7</th>\n",
       "      <th>p54.162.8</th>\n",
       "      <th>...</th>\n",
       "      <th>v100.16</th>\n",
       "      <th>v100.17</th>\n",
       "      <th>v100.18</th>\n",
       "      <th>v100.19</th>\n",
       "      <th>v100.20</th>\n",
       "      <th>v100.21</th>\n",
       "      <th>v100.22</th>\n",
       "      <th>v100.23</th>\n",
       "      <th>v100.24</th>\n",
       "      <th>v100.25</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>402.71</td>\n",
       "      <td>2005</td>\n",
       "      <td>2.534970e+06</td>\n",
       "      <td>2.526864e+06</td>\n",
       "      <td>2.518754e+06</td>\n",
       "      <td>2.510648e+06</td>\n",
       "      <td>2.502537e+06</td>\n",
       "      <td>2.531111e+06</td>\n",
       "      <td>2.522721e+06</td>\n",
       "      <td>2.514330e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.683596</td>\n",
       "      <td>-4.545396</td>\n",
       "      <td>-4.407196</td>\n",
       "      <td>-4.268996</td>\n",
       "      <td>-4.131295</td>\n",
       "      <td>-4.669626</td>\n",
       "      <td>-4.528932</td>\n",
       "      <td>-4.388736</td>\n",
       "      <td>-4.248540</td>\n",
       "      <td>-4.107846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>696.80</td>\n",
       "      <td>2005</td>\n",
       "      <td>2.537369e+06</td>\n",
       "      <td>2.529277e+06</td>\n",
       "      <td>2.521184e+06</td>\n",
       "      <td>2.513088e+06</td>\n",
       "      <td>2.504995e+06</td>\n",
       "      <td>2.533465e+06</td>\n",
       "      <td>2.525088e+06</td>\n",
       "      <td>2.516716e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.397886</td>\n",
       "      <td>-3.257192</td>\n",
       "      <td>-3.115998</td>\n",
       "      <td>-2.975304</td>\n",
       "      <td>-2.834609</td>\n",
       "      <td>-3.396390</td>\n",
       "      <td>-3.254198</td>\n",
       "      <td>-3.112506</td>\n",
       "      <td>-2.970314</td>\n",
       "      <td>-2.828622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1591.15</td>\n",
       "      <td>2005</td>\n",
       "      <td>2.533727e+06</td>\n",
       "      <td>2.525703e+06</td>\n",
       "      <td>2.517678e+06</td>\n",
       "      <td>2.509654e+06</td>\n",
       "      <td>2.501629e+06</td>\n",
       "      <td>2.529801e+06</td>\n",
       "      <td>2.521496e+06</td>\n",
       "      <td>2.513187e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.454105</td>\n",
       "      <td>-1.296447</td>\n",
       "      <td>-1.138290</td>\n",
       "      <td>-0.980134</td>\n",
       "      <td>-0.822476</td>\n",
       "      <td>-1.459094</td>\n",
       "      <td>-1.302933</td>\n",
       "      <td>-1.147271</td>\n",
       "      <td>-0.991110</td>\n",
       "      <td>-0.834949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1338.62</td>\n",
       "      <td>2005</td>\n",
       "      <td>2.534491e+06</td>\n",
       "      <td>2.526548e+06</td>\n",
       "      <td>2.518609e+06</td>\n",
       "      <td>2.510670e+06</td>\n",
       "      <td>2.502732e+06</td>\n",
       "      <td>2.530569e+06</td>\n",
       "      <td>2.522346e+06</td>\n",
       "      <td>2.514127e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>1.255015</td>\n",
       "      <td>1.370265</td>\n",
       "      <td>1.485515</td>\n",
       "      <td>1.600765</td>\n",
       "      <td>1.716015</td>\n",
       "      <td>1.210612</td>\n",
       "      <td>1.319376</td>\n",
       "      <td>1.428140</td>\n",
       "      <td>1.536405</td>\n",
       "      <td>1.645169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>562.50</td>\n",
       "      <td>2005</td>\n",
       "      <td>2.529543e+06</td>\n",
       "      <td>2.521623e+06</td>\n",
       "      <td>2.513702e+06</td>\n",
       "      <td>2.505782e+06</td>\n",
       "      <td>2.497861e+06</td>\n",
       "      <td>2.525621e+06</td>\n",
       "      <td>2.517421e+06</td>\n",
       "      <td>2.509215e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>1.939031</td>\n",
       "      <td>2.023847</td>\n",
       "      <td>2.108663</td>\n",
       "      <td>2.193977</td>\n",
       "      <td>2.278793</td>\n",
       "      <td>1.873673</td>\n",
       "      <td>1.953000</td>\n",
       "      <td>2.031829</td>\n",
       "      <td>2.111157</td>\n",
       "      <td>2.189986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5932</th>\n",
       "      <td>211.78</td>\n",
       "      <td>2010</td>\n",
       "      <td>2.450279e+06</td>\n",
       "      <td>2.442801e+06</td>\n",
       "      <td>2.435324e+06</td>\n",
       "      <td>2.427846e+06</td>\n",
       "      <td>2.420368e+06</td>\n",
       "      <td>2.446240e+06</td>\n",
       "      <td>2.438486e+06</td>\n",
       "      <td>2.430738e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>3.473201</td>\n",
       "      <td>3.445761</td>\n",
       "      <td>3.418819</td>\n",
       "      <td>3.391379</td>\n",
       "      <td>3.363938</td>\n",
       "      <td>3.499644</td>\n",
       "      <td>3.467214</td>\n",
       "      <td>3.434785</td>\n",
       "      <td>3.401856</td>\n",
       "      <td>3.369426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5933</th>\n",
       "      <td>944.52</td>\n",
       "      <td>2010</td>\n",
       "      <td>2.455407e+06</td>\n",
       "      <td>2.447817e+06</td>\n",
       "      <td>2.440226e+06</td>\n",
       "      <td>2.432635e+06</td>\n",
       "      <td>2.425045e+06</td>\n",
       "      <td>2.451400e+06</td>\n",
       "      <td>2.443533e+06</td>\n",
       "      <td>2.435667e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>2.280789</td>\n",
       "      <td>2.372091</td>\n",
       "      <td>2.463892</td>\n",
       "      <td>2.555194</td>\n",
       "      <td>2.646994</td>\n",
       "      <td>2.333674</td>\n",
       "      <td>2.418490</td>\n",
       "      <td>2.503306</td>\n",
       "      <td>2.588621</td>\n",
       "      <td>2.673437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5934</th>\n",
       "      <td>224.06</td>\n",
       "      <td>2010</td>\n",
       "      <td>2.457296e+06</td>\n",
       "      <td>2.449624e+06</td>\n",
       "      <td>2.441947e+06</td>\n",
       "      <td>2.434271e+06</td>\n",
       "      <td>2.426599e+06</td>\n",
       "      <td>2.453288e+06</td>\n",
       "      <td>2.445341e+06</td>\n",
       "      <td>2.437393e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>2.211939</td>\n",
       "      <td>2.341657</td>\n",
       "      <td>2.470877</td>\n",
       "      <td>2.600096</td>\n",
       "      <td>2.729815</td>\n",
       "      <td>2.188489</td>\n",
       "      <td>2.312221</td>\n",
       "      <td>2.436451</td>\n",
       "      <td>2.560183</td>\n",
       "      <td>2.684413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5935</th>\n",
       "      <td>0.37</td>\n",
       "      <td>2010</td>\n",
       "      <td>2.464015e+06</td>\n",
       "      <td>2.456257e+06</td>\n",
       "      <td>2.448494e+06</td>\n",
       "      <td>2.440732e+06</td>\n",
       "      <td>2.432974e+06</td>\n",
       "      <td>2.459993e+06</td>\n",
       "      <td>2.451955e+06</td>\n",
       "      <td>2.443917e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>1.235059</td>\n",
       "      <td>1.311393</td>\n",
       "      <td>1.387228</td>\n",
       "      <td>1.463563</td>\n",
       "      <td>1.539897</td>\n",
       "      <td>1.263996</td>\n",
       "      <td>1.338335</td>\n",
       "      <td>1.412174</td>\n",
       "      <td>1.486513</td>\n",
       "      <td>1.560852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5936</th>\n",
       "      <td>70.14</td>\n",
       "      <td>2010</td>\n",
       "      <td>2.468054e+06</td>\n",
       "      <td>2.460197e+06</td>\n",
       "      <td>2.452335e+06</td>\n",
       "      <td>2.444478e+06</td>\n",
       "      <td>2.436616e+06</td>\n",
       "      <td>2.464019e+06</td>\n",
       "      <td>2.455882e+06</td>\n",
       "      <td>2.447749e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.887335</td>\n",
       "      <td>-0.762107</td>\n",
       "      <td>-0.636879</td>\n",
       "      <td>-0.512149</td>\n",
       "      <td>-0.386921</td>\n",
       "      <td>-0.840437</td>\n",
       "      <td>-0.716705</td>\n",
       "      <td>-0.592974</td>\n",
       "      <td>-0.469242</td>\n",
       "      <td>-0.345511</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5937 rows × 552 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       energy  year     p54.162.1     p54.162.2     p54.162.3     p54.162.4  \\\n",
       "0      402.71  2005  2.534970e+06  2.526864e+06  2.518754e+06  2.510648e+06   \n",
       "1      696.80  2005  2.537369e+06  2.529277e+06  2.521184e+06  2.513088e+06   \n",
       "2     1591.15  2005  2.533727e+06  2.525703e+06  2.517678e+06  2.509654e+06   \n",
       "3     1338.62  2005  2.534491e+06  2.526548e+06  2.518609e+06  2.510670e+06   \n",
       "4      562.50  2005  2.529543e+06  2.521623e+06  2.513702e+06  2.505782e+06   \n",
       "...       ...   ...           ...           ...           ...           ...   \n",
       "5932   211.78  2010  2.450279e+06  2.442801e+06  2.435324e+06  2.427846e+06   \n",
       "5933   944.52  2010  2.455407e+06  2.447817e+06  2.440226e+06  2.432635e+06   \n",
       "5934   224.06  2010  2.457296e+06  2.449624e+06  2.441947e+06  2.434271e+06   \n",
       "5935     0.37  2010  2.464015e+06  2.456257e+06  2.448494e+06  2.440732e+06   \n",
       "5936    70.14  2010  2.468054e+06  2.460197e+06  2.452335e+06  2.444478e+06   \n",
       "\n",
       "         p54.162.5     p54.162.6     p54.162.7     p54.162.8  ...   v100.16  \\\n",
       "0     2.502537e+06  2.531111e+06  2.522721e+06  2.514330e+06  ... -4.683596   \n",
       "1     2.504995e+06  2.533465e+06  2.525088e+06  2.516716e+06  ... -3.397886   \n",
       "2     2.501629e+06  2.529801e+06  2.521496e+06  2.513187e+06  ... -1.454105   \n",
       "3     2.502732e+06  2.530569e+06  2.522346e+06  2.514127e+06  ...  1.255015   \n",
       "4     2.497861e+06  2.525621e+06  2.517421e+06  2.509215e+06  ...  1.939031   \n",
       "...            ...           ...           ...           ...  ...       ...   \n",
       "5932  2.420368e+06  2.446240e+06  2.438486e+06  2.430738e+06  ...  3.473201   \n",
       "5933  2.425045e+06  2.451400e+06  2.443533e+06  2.435667e+06  ...  2.280789   \n",
       "5934  2.426599e+06  2.453288e+06  2.445341e+06  2.437393e+06  ...  2.211939   \n",
       "5935  2.432974e+06  2.459993e+06  2.451955e+06  2.443917e+06  ...  1.235059   \n",
       "5936  2.436616e+06  2.464019e+06  2.455882e+06  2.447749e+06  ... -0.887335   \n",
       "\n",
       "       v100.17   v100.18   v100.19   v100.20   v100.21   v100.22   v100.23  \\\n",
       "0    -4.545396 -4.407196 -4.268996 -4.131295 -4.669626 -4.528932 -4.388736   \n",
       "1    -3.257192 -3.115998 -2.975304 -2.834609 -3.396390 -3.254198 -3.112506   \n",
       "2    -1.296447 -1.138290 -0.980134 -0.822476 -1.459094 -1.302933 -1.147271   \n",
       "3     1.370265  1.485515  1.600765  1.716015  1.210612  1.319376  1.428140   \n",
       "4     2.023847  2.108663  2.193977  2.278793  1.873673  1.953000  2.031829   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "5932  3.445761  3.418819  3.391379  3.363938  3.499644  3.467214  3.434785   \n",
       "5933  2.372091  2.463892  2.555194  2.646994  2.333674  2.418490  2.503306   \n",
       "5934  2.341657  2.470877  2.600096  2.729815  2.188489  2.312221  2.436451   \n",
       "5935  1.311393  1.387228  1.463563  1.539897  1.263996  1.338335  1.412174   \n",
       "5936 -0.762107 -0.636879 -0.512149 -0.386921 -0.840437 -0.716705 -0.592974   \n",
       "\n",
       "       v100.24   v100.25  \n",
       "0    -4.248540 -4.107846  \n",
       "1    -2.970314 -2.828622  \n",
       "2    -0.991110 -0.834949  \n",
       "3     1.536405  1.645169  \n",
       "4     2.111157  2.189986  \n",
       "...        ...       ...  \n",
       "5932  3.401856  3.369426  \n",
       "5933  2.588621  2.673437  \n",
       "5934  2.560183  2.684413  \n",
       "5935  1.486513  1.560852  \n",
       "5936 -0.469242 -0.345511  \n",
       "\n",
       "[5937 rows x 552 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we have removed the four non-usable attributes. Let's try with the drop() method, which is quite more simpler. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle('wind_pickle.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>energy</th>\n",
       "      <th>year</th>\n",
       "      <th>p54.162.1</th>\n",
       "      <th>p54.162.2</th>\n",
       "      <th>p54.162.3</th>\n",
       "      <th>p54.162.4</th>\n",
       "      <th>p54.162.5</th>\n",
       "      <th>p54.162.6</th>\n",
       "      <th>p54.162.7</th>\n",
       "      <th>p54.162.8</th>\n",
       "      <th>...</th>\n",
       "      <th>v100.16</th>\n",
       "      <th>v100.17</th>\n",
       "      <th>v100.18</th>\n",
       "      <th>v100.19</th>\n",
       "      <th>v100.20</th>\n",
       "      <th>v100.21</th>\n",
       "      <th>v100.22</th>\n",
       "      <th>v100.23</th>\n",
       "      <th>v100.24</th>\n",
       "      <th>v100.25</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>402.71</td>\n",
       "      <td>2005</td>\n",
       "      <td>2.534970e+06</td>\n",
       "      <td>2.526864e+06</td>\n",
       "      <td>2.518754e+06</td>\n",
       "      <td>2.510648e+06</td>\n",
       "      <td>2.502537e+06</td>\n",
       "      <td>2.531111e+06</td>\n",
       "      <td>2.522721e+06</td>\n",
       "      <td>2.514330e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.683596</td>\n",
       "      <td>-4.545396</td>\n",
       "      <td>-4.407196</td>\n",
       "      <td>-4.268996</td>\n",
       "      <td>-4.131295</td>\n",
       "      <td>-4.669626</td>\n",
       "      <td>-4.528932</td>\n",
       "      <td>-4.388736</td>\n",
       "      <td>-4.248540</td>\n",
       "      <td>-4.107846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>696.80</td>\n",
       "      <td>2005</td>\n",
       "      <td>2.537369e+06</td>\n",
       "      <td>2.529277e+06</td>\n",
       "      <td>2.521184e+06</td>\n",
       "      <td>2.513088e+06</td>\n",
       "      <td>2.504995e+06</td>\n",
       "      <td>2.533465e+06</td>\n",
       "      <td>2.525088e+06</td>\n",
       "      <td>2.516716e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.397886</td>\n",
       "      <td>-3.257192</td>\n",
       "      <td>-3.115998</td>\n",
       "      <td>-2.975304</td>\n",
       "      <td>-2.834609</td>\n",
       "      <td>-3.396390</td>\n",
       "      <td>-3.254198</td>\n",
       "      <td>-3.112506</td>\n",
       "      <td>-2.970314</td>\n",
       "      <td>-2.828622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1591.15</td>\n",
       "      <td>2005</td>\n",
       "      <td>2.533727e+06</td>\n",
       "      <td>2.525703e+06</td>\n",
       "      <td>2.517678e+06</td>\n",
       "      <td>2.509654e+06</td>\n",
       "      <td>2.501629e+06</td>\n",
       "      <td>2.529801e+06</td>\n",
       "      <td>2.521496e+06</td>\n",
       "      <td>2.513187e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.454105</td>\n",
       "      <td>-1.296447</td>\n",
       "      <td>-1.138290</td>\n",
       "      <td>-0.980134</td>\n",
       "      <td>-0.822476</td>\n",
       "      <td>-1.459094</td>\n",
       "      <td>-1.302933</td>\n",
       "      <td>-1.147271</td>\n",
       "      <td>-0.991110</td>\n",
       "      <td>-0.834949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1338.62</td>\n",
       "      <td>2005</td>\n",
       "      <td>2.534491e+06</td>\n",
       "      <td>2.526548e+06</td>\n",
       "      <td>2.518609e+06</td>\n",
       "      <td>2.510670e+06</td>\n",
       "      <td>2.502732e+06</td>\n",
       "      <td>2.530569e+06</td>\n",
       "      <td>2.522346e+06</td>\n",
       "      <td>2.514127e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>1.255015</td>\n",
       "      <td>1.370265</td>\n",
       "      <td>1.485515</td>\n",
       "      <td>1.600765</td>\n",
       "      <td>1.716015</td>\n",
       "      <td>1.210612</td>\n",
       "      <td>1.319376</td>\n",
       "      <td>1.428140</td>\n",
       "      <td>1.536405</td>\n",
       "      <td>1.645169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>562.50</td>\n",
       "      <td>2005</td>\n",
       "      <td>2.529543e+06</td>\n",
       "      <td>2.521623e+06</td>\n",
       "      <td>2.513702e+06</td>\n",
       "      <td>2.505782e+06</td>\n",
       "      <td>2.497861e+06</td>\n",
       "      <td>2.525621e+06</td>\n",
       "      <td>2.517421e+06</td>\n",
       "      <td>2.509215e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>1.939031</td>\n",
       "      <td>2.023847</td>\n",
       "      <td>2.108663</td>\n",
       "      <td>2.193977</td>\n",
       "      <td>2.278793</td>\n",
       "      <td>1.873673</td>\n",
       "      <td>1.953000</td>\n",
       "      <td>2.031829</td>\n",
       "      <td>2.111157</td>\n",
       "      <td>2.189986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5932</th>\n",
       "      <td>211.78</td>\n",
       "      <td>2010</td>\n",
       "      <td>2.450279e+06</td>\n",
       "      <td>2.442801e+06</td>\n",
       "      <td>2.435324e+06</td>\n",
       "      <td>2.427846e+06</td>\n",
       "      <td>2.420368e+06</td>\n",
       "      <td>2.446240e+06</td>\n",
       "      <td>2.438486e+06</td>\n",
       "      <td>2.430738e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>3.473201</td>\n",
       "      <td>3.445761</td>\n",
       "      <td>3.418819</td>\n",
       "      <td>3.391379</td>\n",
       "      <td>3.363938</td>\n",
       "      <td>3.499644</td>\n",
       "      <td>3.467214</td>\n",
       "      <td>3.434785</td>\n",
       "      <td>3.401856</td>\n",
       "      <td>3.369426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5933</th>\n",
       "      <td>944.52</td>\n",
       "      <td>2010</td>\n",
       "      <td>2.455407e+06</td>\n",
       "      <td>2.447817e+06</td>\n",
       "      <td>2.440226e+06</td>\n",
       "      <td>2.432635e+06</td>\n",
       "      <td>2.425045e+06</td>\n",
       "      <td>2.451400e+06</td>\n",
       "      <td>2.443533e+06</td>\n",
       "      <td>2.435667e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>2.280789</td>\n",
       "      <td>2.372091</td>\n",
       "      <td>2.463892</td>\n",
       "      <td>2.555194</td>\n",
       "      <td>2.646994</td>\n",
       "      <td>2.333674</td>\n",
       "      <td>2.418490</td>\n",
       "      <td>2.503306</td>\n",
       "      <td>2.588621</td>\n",
       "      <td>2.673437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5934</th>\n",
       "      <td>224.06</td>\n",
       "      <td>2010</td>\n",
       "      <td>2.457296e+06</td>\n",
       "      <td>2.449624e+06</td>\n",
       "      <td>2.441947e+06</td>\n",
       "      <td>2.434271e+06</td>\n",
       "      <td>2.426599e+06</td>\n",
       "      <td>2.453288e+06</td>\n",
       "      <td>2.445341e+06</td>\n",
       "      <td>2.437393e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>2.211939</td>\n",
       "      <td>2.341657</td>\n",
       "      <td>2.470877</td>\n",
       "      <td>2.600096</td>\n",
       "      <td>2.729815</td>\n",
       "      <td>2.188489</td>\n",
       "      <td>2.312221</td>\n",
       "      <td>2.436451</td>\n",
       "      <td>2.560183</td>\n",
       "      <td>2.684413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5935</th>\n",
       "      <td>0.37</td>\n",
       "      <td>2010</td>\n",
       "      <td>2.464015e+06</td>\n",
       "      <td>2.456257e+06</td>\n",
       "      <td>2.448494e+06</td>\n",
       "      <td>2.440732e+06</td>\n",
       "      <td>2.432974e+06</td>\n",
       "      <td>2.459993e+06</td>\n",
       "      <td>2.451955e+06</td>\n",
       "      <td>2.443917e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>1.235059</td>\n",
       "      <td>1.311393</td>\n",
       "      <td>1.387228</td>\n",
       "      <td>1.463563</td>\n",
       "      <td>1.539897</td>\n",
       "      <td>1.263996</td>\n",
       "      <td>1.338335</td>\n",
       "      <td>1.412174</td>\n",
       "      <td>1.486513</td>\n",
       "      <td>1.560852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5936</th>\n",
       "      <td>70.14</td>\n",
       "      <td>2010</td>\n",
       "      <td>2.468054e+06</td>\n",
       "      <td>2.460197e+06</td>\n",
       "      <td>2.452335e+06</td>\n",
       "      <td>2.444478e+06</td>\n",
       "      <td>2.436616e+06</td>\n",
       "      <td>2.464019e+06</td>\n",
       "      <td>2.455882e+06</td>\n",
       "      <td>2.447749e+06</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.887335</td>\n",
       "      <td>-0.762107</td>\n",
       "      <td>-0.636879</td>\n",
       "      <td>-0.512149</td>\n",
       "      <td>-0.386921</td>\n",
       "      <td>-0.840437</td>\n",
       "      <td>-0.716705</td>\n",
       "      <td>-0.592974</td>\n",
       "      <td>-0.469242</td>\n",
       "      <td>-0.345511</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5937 rows × 552 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       energy  year     p54.162.1     p54.162.2     p54.162.3     p54.162.4  \\\n",
       "0      402.71  2005  2.534970e+06  2.526864e+06  2.518754e+06  2.510648e+06   \n",
       "1      696.80  2005  2.537369e+06  2.529277e+06  2.521184e+06  2.513088e+06   \n",
       "2     1591.15  2005  2.533727e+06  2.525703e+06  2.517678e+06  2.509654e+06   \n",
       "3     1338.62  2005  2.534491e+06  2.526548e+06  2.518609e+06  2.510670e+06   \n",
       "4      562.50  2005  2.529543e+06  2.521623e+06  2.513702e+06  2.505782e+06   \n",
       "...       ...   ...           ...           ...           ...           ...   \n",
       "5932   211.78  2010  2.450279e+06  2.442801e+06  2.435324e+06  2.427846e+06   \n",
       "5933   944.52  2010  2.455407e+06  2.447817e+06  2.440226e+06  2.432635e+06   \n",
       "5934   224.06  2010  2.457296e+06  2.449624e+06  2.441947e+06  2.434271e+06   \n",
       "5935     0.37  2010  2.464015e+06  2.456257e+06  2.448494e+06  2.440732e+06   \n",
       "5936    70.14  2010  2.468054e+06  2.460197e+06  2.452335e+06  2.444478e+06   \n",
       "\n",
       "         p54.162.5     p54.162.6     p54.162.7     p54.162.8  ...   v100.16  \\\n",
       "0     2.502537e+06  2.531111e+06  2.522721e+06  2.514330e+06  ... -4.683596   \n",
       "1     2.504995e+06  2.533465e+06  2.525088e+06  2.516716e+06  ... -3.397886   \n",
       "2     2.501629e+06  2.529801e+06  2.521496e+06  2.513187e+06  ... -1.454105   \n",
       "3     2.502732e+06  2.530569e+06  2.522346e+06  2.514127e+06  ...  1.255015   \n",
       "4     2.497861e+06  2.525621e+06  2.517421e+06  2.509215e+06  ...  1.939031   \n",
       "...            ...           ...           ...           ...  ...       ...   \n",
       "5932  2.420368e+06  2.446240e+06  2.438486e+06  2.430738e+06  ...  3.473201   \n",
       "5933  2.425045e+06  2.451400e+06  2.443533e+06  2.435667e+06  ...  2.280789   \n",
       "5934  2.426599e+06  2.453288e+06  2.445341e+06  2.437393e+06  ...  2.211939   \n",
       "5935  2.432974e+06  2.459993e+06  2.451955e+06  2.443917e+06  ...  1.235059   \n",
       "5936  2.436616e+06  2.464019e+06  2.455882e+06  2.447749e+06  ... -0.887335   \n",
       "\n",
       "       v100.17   v100.18   v100.19   v100.20   v100.21   v100.22   v100.23  \\\n",
       "0    -4.545396 -4.407196 -4.268996 -4.131295 -4.669626 -4.528932 -4.388736   \n",
       "1    -3.257192 -3.115998 -2.975304 -2.834609 -3.396390 -3.254198 -3.112506   \n",
       "2    -1.296447 -1.138290 -0.980134 -0.822476 -1.459094 -1.302933 -1.147271   \n",
       "3     1.370265  1.485515  1.600765  1.716015  1.210612  1.319376  1.428140   \n",
       "4     2.023847  2.108663  2.193977  2.278793  1.873673  1.953000  2.031829   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "5932  3.445761  3.418819  3.391379  3.363938  3.499644  3.467214  3.434785   \n",
       "5933  2.372091  2.463892  2.555194  2.646994  2.333674  2.418490  2.503306   \n",
       "5934  2.341657  2.470877  2.600096  2.729815  2.188489  2.312221  2.436451   \n",
       "5935  1.311393  1.387228  1.463563  1.539897  1.263996  1.338335  1.412174   \n",
       "5936 -0.762107 -0.636879 -0.512149 -0.386921 -0.840437 -0.716705 -0.592974   \n",
       "\n",
       "       v100.24   v100.25  \n",
       "0    -4.248540 -4.107846  \n",
       "1    -2.970314 -2.828622  \n",
       "2    -0.991110 -0.834949  \n",
       "3     1.536405  1.645169  \n",
       "4     2.111157  2.189986  \n",
       "...        ...       ...  \n",
       "5932  3.401856  3.369426  \n",
       "5933  2.588621  2.673437  \n",
       "5934  2.560183  2.684413  \n",
       "5935  1.486513  1.560852  \n",
       "5936 -0.469242 -0.345511  \n",
       "\n",
       "[5937 rows x 552 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.drop(['steps', 'month', 'day','hour'], axis=1)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, this method is checked and it is valid. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we introduce the seed to avoid reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import random\n",
    "my_NIA = 100441742 \n",
    "random.seed(my_NIA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to chose randomly a 10% of the columns of the data set (without including the first two variables: year and energy). We are labelling the number of columns to select randomly as n. Then, we have to raplace randomly a 5% of the values of each of these columns by a missing value (introduced in Python as np.nan). We are labelling this number as m, which is constructed as the 5% of the observations of each columns (i.e. 5% of the total number of observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(len(list(data)[2:])*0.1)\n",
    "m = round(data.shape[0]*0.05) # To introduce at least a 5%. It is going to be a little more, since the 5% corresponds to 296.85."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we have the number of columns that we have to select (n = 55) and the number of observations of each of them that we need to replace by a missing value. We will transform the 55 columns to 55 numbers of columns selected at random from the list of labels (excluding the first two of them that correspond to the variables year and energy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import sample\n",
    "from random import randint\n",
    "\n",
    "random.seed(my_NIA)\n",
    "rannames = sample(list(data)[2:],k = n)\n",
    "\n",
    "for i in range(len(rannames)):\n",
    "    random.seed(my_NIA*i)\n",
    "    randrows = random.sample(range(data.shape[0]),m)\n",
    "    for row in range(m):\n",
    "        data.loc[randrows[row],rannames[i]] = np.nan "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this procedure, we iterate over the 55 columns selected at random. For each column, we create a random sample of 297 (m) elements at random and without replacement. We have done this under a seed (different for each column to not select the same elements for each column) in order to keep reproducibility. We then iterate over that column to transform the randomly selected 5% of its elements to missing values. After doing this, it is interesting to check the number of missing values introduced in our data set. It has to be equal to 55 x 297 = 16335 (which is the result we obtain)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of missing values is: 16335\n"
     ]
    }
   ],
   "source": [
    "print('The number of missing values is: {0}'.format(data.isna().sum().sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After having introduced the missing values, we have to create three partitions of the data set. This is not going tobe done randomly since we have data belonging to consecutive hours. If we done this partition randomly, we might find the situation where the model in trained with similar data that then we use to validate it. This will lead in overestimated measures of quality of our model. Therefore, we are said to form these partitions considering data belonging to separated years. After forming the three partitions, we remove the variable year, which is no longer useful. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition1 = (data.year == 2005)|(data.year == 2006)\n",
    "train = data.loc[condition1]\n",
    "\n",
    "condition2 = (data.year == 2007)|(data.year == 2008)\n",
    "valpart = data.loc[condition2]\n",
    "\n",
    "condition3 = (data.year == 2009)|(data.year == 2010)\n",
    "test = data.loc[condition3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.drop(['year'], axis='columns')\n",
    "\n",
    "test = test.drop(['year'], axis='columns')\n",
    "\n",
    "valpart = valpart.drop(['year'], axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dimensions of the training partition are: (2528, 551)\n",
      "The dimensions of the testing partition are: (2110, 551)\n",
      "The dimensions of the validation partition are: (1299, 551)\n"
     ]
    }
   ],
   "source": [
    "print(\"The dimensions of the training partition are: {0}\".format(train.shape))\n",
    "\n",
    "print(\"The dimensions of the testing partition are: {0}\".format(test.shape))\n",
    "\n",
    "print(\"The dimensions of the validation partition are: {0}\".format(valpart.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model selection and hyper-parameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin this part creating the training and the testing sets out of our data. As commented before, the sets are not formed randomly but are separated manually. This implies that we just need to separate the response variable and the attributes. The first one corresponds to the \"y\"; and the set of attributes to \"X\". We do this process both for the training and testing sets, which are going to be used in this section (in which we do not need to tune the hyper-parameters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train.iloc[:,1:]\n",
    "y_train = train.iloc[:,0]\n",
    "X_test = test.iloc[:,1:]\n",
    "y_test = test.iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we define the methods that will construct our pipeline. First, we want to impute the missing values (that we introduced in the first part of the assignment). For this purpose, we are using the 'mean' strategy. Then, we want to scale the parameters, which we are doing by standarization (substracting the mean and dividing by the standard deviation). The first method computes the mean of each attribute and substitute the missing values by those means. It saves the values of the means, which are used also for the second method of standarization. Once the pre-process is done (using only the training data), we want to fit a regression model using the knn method. We define each of those methods and combine them into a pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "scaler = StandardScaler()\n",
    "knn = KNeighborsRegressor()\n",
    "\n",
    "knn_pipe = Pipeline([('imputer', imputer),('scaler',scaler),('knn_regression',knn)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are ready to fit the pipeline with the training data and predict the values of the response variable (energy) for the characteristics of the observations left as testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MAE for knn without tuning is =  330.4147\n"
     ]
    }
   ],
   "source": [
    "knn_pipe.fit(X_train, y_train)\n",
    "y_test_pred0 = knn_pipe.predict(X_test)\n",
    "\n",
    "from sklearn import metrics\n",
    "print(\"The MAE for knn without tuning is = \",round(metrics.mean_absolute_error(y_test, y_test_pred0),4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For SVM, we will use the same pre-process part of the pipeline, since this method also requires scaling and imputation of missing values. However, we change the method of fitting the regression model: now we use SVM instead of KNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MAE for SVM without tuning is =  503.1826\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "SVM = SVR()\n",
    "svm_pipe = Pipeline([('imputer', imputer),('scaler',scaler),('SVM',SVM)])\n",
    "svm_pipe.fit(X_train, y_train)\n",
    "y_test_pred1 = svm_pipe.predict(X_test)\n",
    "\n",
    "print(\"The MAE for SVM without tuning is = \",round(metrics.mean_absolute_error(y_test, y_test_pred1),4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe quite high value of the MSE in comparison with the knn method. The SVM is more complex that KNN, which can be a reason why the KNN without tuning works better than this method. Being more complex means having more hyper-parameters to tune. Obviously, working without tuning the parameters is worse for methods with more parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this part, since we do not need to scale, we will impute the missing values and fit a regression model with the decision trees method directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MAE for Trees without tuning is =  392.4143\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree = DecisionTreeRegressor()\n",
    "\n",
    "tree_pipe = Pipeline([('imputer', imputer),('tree',tree)])\n",
    "tree_pipe.fit(X_train, y_train)\n",
    "y_test_pred2 = tree_pipe.predict(X_test)\n",
    "\n",
    "print(\"The MAE for Trees without tuning is = \",round(metrics.mean_absolute_error(y_test, y_test_pred2),4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we have to create the X and the y of the validation partition that we separated previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valpart = valpart.iloc[:,1:]\n",
    "y_valpart = valpart.iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, we define the pipeline that must be used. In fact, it is the same one as before with some particular differences. Now we do not introduce the method of imputation ('mean' or 'median') but left it as a hyper-parameter to be tuned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('imputer', SimpleImputer()),\n",
       "  ('scaler', StandardScaler()),\n",
       "  ('knn_regression', KNeighborsRegressor())],\n",
       " 'verbose': False,\n",
       " 'imputer': SimpleImputer(),\n",
       " 'scaler': StandardScaler(),\n",
       " 'knn_regression': KNeighborsRegressor(),\n",
       " 'imputer__add_indicator': False,\n",
       " 'imputer__copy': True,\n",
       " 'imputer__fill_value': None,\n",
       " 'imputer__missing_values': nan,\n",
       " 'imputer__strategy': 'mean',\n",
       " 'imputer__verbose': 0,\n",
       " 'scaler__copy': True,\n",
       " 'scaler__with_mean': True,\n",
       " 'scaler__with_std': True,\n",
       " 'knn_regression__algorithm': 'auto',\n",
       " 'knn_regression__leaf_size': 30,\n",
       " 'knn_regression__metric': 'minkowski',\n",
       " 'knn_regression__metric_params': None,\n",
       " 'knn_regression__n_jobs': None,\n",
       " 'knn_regression__n_neighbors': 5,\n",
       " 'knn_regression__p': 2,\n",
       " 'knn_regression__weights': 'uniform'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imputer = SimpleImputer()\n",
    "scaler = StandardScaler()\n",
    "knn = KNeighborsRegressor()\n",
    "\n",
    "knn_pipe = Pipeline([('imputer', imputer),('scaler',scaler),('knn_regression',knn)])\n",
    "knn_pipe.get_params() # we check the default parameters in order to check how they must be referred to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, it is necessary to define the search space. For this purpose, we checked the hyper-parameters of the pipeline, since we must know how they are named. We are particularly interested on: 'knn_regression__n_neighbors' (the number of neighbors of the knn method); and 'imputer__strategy' (whether we use the mean or the median for the imputation of the missing values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'imputer__strategy':['mean','median'],'knn_regression__n_neighbors': [2,3,4,5,6,7,8,9,10,11]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before defining the grid for inner validation (tuning of the parameters), we must divide the validation partition into a training and a testing sets. We will include 2/3 of the observations for training and the rest of them for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import PredefinedSplit\n",
    "\n",
    "validation_indices = np.zeros(X_valpart.shape[0])\n",
    "validation_indices[:round(2/3*X_valpart.shape[0])] = -1\n",
    "\n",
    "tr_valpart = PredefinedSplit(validation_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to create the grid for inner validation. We introduce the pipeline, the search space, the scoring method (which is going to be NMAE -> we use negative MAE because is a scoring method, which tends to be maximized. However, our metric is an error, that should be minimized. That is why we must use the negative one), the cross validation set (the 2/3 of the data of the validation partition), and other parameters as we did in theory lessons. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "knn_grid = RandomizedSearchCV(knn_pipe, param_grid,\n",
    "                              scoring = 'neg_mean_absolute_error',\n",
    "                              cv = tr_valpart, \n",
    "                              n_jobs=2, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are ready to fit the model with the best parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 10 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
      "[Parallel(n_jobs=2)]: Done  10 out of  10 | elapsed:    3.5s finished\n"
     ]
    }
   ],
   "source": [
    "random.seed(my_NIA)\n",
    "knn_grid = knn_grid.fit(X_train, y_train)\n",
    "\n",
    "y_test_pred3 = knn_grid.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'knn_regression__n_neighbors': 6, 'imputer__strategy': 'median'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_grid.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We calculate the MAE for the tunned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MAE for knn tuning the hyper-parameters is =  327.307\n"
     ]
    }
   ],
   "source": [
    "print(\"The MAE for knn tuning the hyper-parameters is = \",round(metrics.mean_absolute_error(y_test, y_test_pred3),4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have reduced the MAE by tuning the hyper-parameters of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('imputer', SimpleImputer()),\n",
       "  ('scaler', StandardScaler()),\n",
       "  ('SVM', SVR())],\n",
       " 'verbose': False,\n",
       " 'imputer': SimpleImputer(),\n",
       " 'scaler': StandardScaler(),\n",
       " 'SVM': SVR(),\n",
       " 'imputer__add_indicator': False,\n",
       " 'imputer__copy': True,\n",
       " 'imputer__fill_value': None,\n",
       " 'imputer__missing_values': nan,\n",
       " 'imputer__strategy': 'mean',\n",
       " 'imputer__verbose': 0,\n",
       " 'scaler__copy': True,\n",
       " 'scaler__with_mean': True,\n",
       " 'scaler__with_std': True,\n",
       " 'SVM__C': 1.0,\n",
       " 'SVM__cache_size': 200,\n",
       " 'SVM__coef0': 0.0,\n",
       " 'SVM__degree': 3,\n",
       " 'SVM__epsilon': 0.1,\n",
       " 'SVM__gamma': 'scale',\n",
       " 'SVM__kernel': 'rbf',\n",
       " 'SVM__max_iter': -1,\n",
       " 'SVM__shrinking': True,\n",
       " 'SVM__tol': 0.001,\n",
       " 'SVM__verbose': False}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imputer = SimpleImputer()\n",
    "scaler = StandardScaler()\n",
    "SVM = SVR()\n",
    "\n",
    "svm_pipe = Pipeline([('imputer', imputer),('scaler',scaler),('SVM',SVM)])\n",
    "svm_pipe.get_params() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After doing a bit of research, we have found that the parameters to tune in SVM are: **kernels**, which can be Gaussian, polynomial or a sigmoid kernel (those are the functions that transforms low-dimensional space into high-dimensional space); **c-parameter** (parameter for regularisation) which is the penalty parameter; and the **Gamma parameter**, which defines how far influences the calculation of plausible line of separation. A typical parameter grid is the following one. In this part, we have run several times the random search grid for the SVM. We have find huge differences when the random search find the 'rbf' method of the kernel and when it finds other different method. The obtained MAE differs from around 310 to more than 400, respectively. Therefore, after this visual inspection, we remove other kernel functions than rbf. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'imputer__strategy':['mean','median'],\n",
    "              'SVM__C': [0.1,1, 10, 100],\n",
    "              'SVM__gamma':[1,0.1,0.01,0.001]}\n",
    "\n",
    "svm_pipe = svm_pipe.set_params(**{'SVM__kernel':'rbf'}) # After having considered several trials\n",
    "\n",
    "# 'SVM__kernel':('rbf', 'poly', 'sigmoid') --> For the first visual inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 10 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    6.8s finished\n"
     ]
    }
   ],
   "source": [
    "svm_grid = RandomizedSearchCV(svm_pipe,param_grid,\n",
    "                              scoring='neg_mean_absolute_error',\n",
    "                              cv = tr_valpart , \n",
    "                              n_jobs=1, verbose=1)\n",
    "\n",
    "svm_grid = svm_grid.fit(X_train, y_train)\n",
    "y_test_pred4 = svm_grid.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MAE for SVMs tuning the hyper-parameters is =  479.0681\n"
     ]
    }
   ],
   "source": [
    "print(\"The MAE for SVMs tuning the hyper-parameters is = \", round(metrics.mean_absolute_error(y_test, y_test_pred4),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'imputer__strategy': 'median', 'SVM__gamma': 0.01, 'SVM__C': 10}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_grid.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note it is better than the result for KNN and it has a huge improvement in comparison with the same method without tuning the parameters. This might be caused due to the fact we have commented previously: SVM has more parameters and then, the results without tuning them, are expected to be worse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Trees "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('imputer', SimpleImputer()),\n",
       "  ('tree_regression', DecisionTreeRegressor())],\n",
       " 'verbose': False,\n",
       " 'imputer': SimpleImputer(),\n",
       " 'tree_regression': DecisionTreeRegressor(),\n",
       " 'imputer__add_indicator': False,\n",
       " 'imputer__copy': True,\n",
       " 'imputer__fill_value': None,\n",
       " 'imputer__missing_values': nan,\n",
       " 'imputer__strategy': 'mean',\n",
       " 'imputer__verbose': 0,\n",
       " 'tree_regression__ccp_alpha': 0.0,\n",
       " 'tree_regression__criterion': 'mse',\n",
       " 'tree_regression__max_depth': None,\n",
       " 'tree_regression__max_features': None,\n",
       " 'tree_regression__max_leaf_nodes': None,\n",
       " 'tree_regression__min_impurity_decrease': 0.0,\n",
       " 'tree_regression__min_impurity_split': None,\n",
       " 'tree_regression__min_samples_leaf': 1,\n",
       " 'tree_regression__min_samples_split': 2,\n",
       " 'tree_regression__min_weight_fraction_leaf': 0.0,\n",
       " 'tree_regression__presort': 'deprecated',\n",
       " 'tree_regression__random_state': None,\n",
       " 'tree_regression__splitter': 'best'}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imputer = SimpleImputer()\n",
    "tree = DecisionTreeRegressor()\n",
    "\n",
    "tree_grid = Pipeline([('imputer', imputer),('tree_regression',tree)])\n",
    "tree_grid.get_params() # we check the default parameters in order to check how they must be referred to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this pipeline, we have to tune the imputation method and the maximum depth of the tree. In addition, we are tunning the minimum value of splits.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'imputer__strategy':['mean','median'],\n",
    "              'tree_regression__max_depth': range(2,16,2),\n",
    "              'tree_regression__min_samples_split': range(2,34,2)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 10 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    2.5s finished\n"
     ]
    }
   ],
   "source": [
    "tree_grid = RandomizedSearchCV(tree_grid,param_grid,\n",
    "                             scoring = 'neg_mean_absolute_error',\n",
    "                             cv = tr_valpart, \n",
    "                             n_jobs = 1, verbose = 1)\n",
    "\n",
    "tree_grid = tree_grid.fit(X_train , y_train)\n",
    "y_test_pred5 = tree_grid.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tree_regression__min_samples_split': 20,\n",
       " 'tree_regression__max_depth': 4,\n",
       " 'imputer__strategy': 'median'}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MAE for Trees tuning the hyper-parameters is =  349.7526\n"
     ]
    }
   ],
   "source": [
    "print(\"The MAE for Trees tuning the hyper-parameters is = \", round(metrics.mean_absolute_error(y_test, y_test_pred5),4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After doing several trials of the random tuning of the parameters, we have found quite different results for the best hyper-parameters. However, the MAE does not seem to vary a lot from 340, which agains supposes an improvement with respect to the regression with the default parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In summary, we have **improved** the three methods of regression **by tunning the hyper-parameters**. The relative improvement of each method is proportional to the number of hyper-parameters. Thus, SVM, which has more hyper-parameters to tune, has a greater relative improvement. KNN method, for its part, has the lowest improvement on the MAE, since this method has only the number of neighbours as hyper-parameter. In addition, we find the **best method after tunning the hyper-parameters is apparently SVM**. Even though we have not studied yet the particular behavior of SVM, we know it is the most complex method between the three that we had to use in the exercise. Therefore, this result is not surprising for us. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attribute selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this section, we need just to use KNN. In the preivous section we did the tune of the KNN parameters, which were one relative to the imputation part of the pipeline and other to the appropriate number of neighbors of the KNN method itself. In particular, we found the 'mean' strategy was the best method for the imputation of missing values; and the most appropriate number of neighbors was 8. In this exercise we will be tuning the number of neighbors again, so we are just fixing the **imputation method to be the mean**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First pipeline: SelectKBest and regression with KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "\n",
    "\n",
    "imputer = SimpleImputer(strategy = 'mean') # we define the imputer with the fixed method\n",
    "scaler = StandardScaler() # we define the scaler method, necessary for KNN\n",
    "selector = SelectKBest(f_regression) # We define the feature selection method\n",
    "knn_regression = KNeighborsRegressor() # Finally we define the fitting method\n",
    "\n",
    "\n",
    "fea_sel_pipe = Pipeline([('imputer',imputer),\n",
    "                         ('scaler',scaler),\n",
    "                        ('selector',selector),\n",
    "                        ('knn_regressor',knn_regression)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('imputer', SimpleImputer()), ('scaler', StandardScaler()),\n",
       "                ('selector',\n",
       "                 SelectKBest(score_func=<function f_regression at 0x000000AF09CAE048>)),\n",
       "                ('knn_regressor', KNeighborsRegressor())])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fea_sel_pipe.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred6 = fea_sel_pipe.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MAE for KNN with feature selection and without tuning the hyper-parameters is =  538.0683\n"
     ]
    }
   ],
   "source": [
    "print(\"The MAE for KNN with feature selection and without tuning the hyper-parameters is = \",\n",
    "      round(metrics.mean_absolute_error(y_test, y_test_pred6),4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we train the pipeline without tuning the parameters. This is not suitable in any case, since the default values are not normally appropriate for each data set. In our case, we have 551 variables (550 attributes). From those all 550 variables, the method has selected just 10 of them. This reduction entails a decrease of 98% of the number of variables used to train the model. We might be losing important information, instead of removing unimportant characteristics (which is the main purpose of feature selection). We observe a huge rise between the MAE for KNN without feature selection (roughly 330) and the MAE for KNN with feature selection (around 540) (both of them computed without tuning the parameters). Of course, **variable selection is not convenient when we do not specify the proper number of variables to be selected**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[75, 76, 77, 78, 80, 81, 82, 85, 86, 90]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected = fea_sel_pipe['selector'].get_support()\n",
    "\n",
    "selected_col = list()\n",
    "for i in range(len(selected)):\n",
    "    if(selected[i] == True):\n",
    "        selected_col.append(i)\n",
    "\n",
    "selected_col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we have extracted the indexes of the variables that have been selected from this method without tuning the parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second pipeline: PCA with KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "imputer = SimpleImputer(strategy = 'mean') # we define the imputer with the fixed method\n",
    "scaler = StandardScaler() # we define the scaler method, necessary for KNN \n",
    "pca = PCA() # We define the Principal Component Analysis method\n",
    "knn_regression = KNeighborsRegressor() # Finally we define the fitting method\n",
    "\n",
    "PCA_pipe = Pipeline([('imputer',imputer),\n",
    "                         ('scaler',scaler),\n",
    "                        ('pca',pca),\n",
    "                        ('knn_regressor',knn_regression)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('imputer', SimpleImputer()), ('scaler', StandardScaler()),\n",
       "                ('pca', PCA()), ('knn_regressor', KNeighborsRegressor())])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PCA_pipe.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred7 = PCA_pipe.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MAE for KNN with PCA and without tuning the hyper-parameters is =  330.4147\n"
     ]
    }
   ],
   "source": [
    "print(\"The MAE for KNN with PCA and without tuning the hyper-parameters is = \",\n",
    "      round(metrics.mean_absolute_error(y_test, y_test_pred7),4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we have defined a pipeline with imputation (using the mean due to the arguments of the previous pipeline), scaling (necessary for KNN), principal component analysis without selecting a particular number of PCs and fitting with KNN method. The MAE we have obtained is exactly the same as we got when using KNN without PCA. Therefore, we assume that setting no value for the number of components to be selected implies using all PCs. We are studying the proportion of variance explained by each of the PCs, to do a \"parameter tuning\" manually. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9831"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(PCA_pipe['pca'].explained_variance_ratio_[0:12]).round(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first 12 PCs give us a proportion of explained deviance of roughly a 98%. This method could be used to fix a god hyper-parameter tuning of the number of PCs to be used without tuning them automatically. Let's run the code once again without tuning the number of PCs and selecting 3 of them (usual number of PCs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = SimpleImputer(strategy = 'mean') # we define the imputer with the fixed method\n",
    "scaler = StandardScaler() # we define the scaler method, necessary for KNN \n",
    "pca = PCA(n_components = 3) # We define the Principal Component Analysis method\n",
    "knn_regression = KNeighborsRegressor() # Finally we define the fitting method\n",
    "\n",
    "PCA_pipe2 = Pipeline([('imputer',imputer),\n",
    "                         ('scaler',scaler),\n",
    "                        ('pca',pca),\n",
    "                        ('knn_regressor',knn_regression)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "PCA_pipe2.fit(X_train,y_train)\n",
    "y_test_pred8 = PCA_pipe2.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MAE for KNN with PCA and without tuning the hyper-parameters (with 3 PCs) is =  365.2589\n"
     ]
    }
   ],
   "source": [
    "print(\"The MAE for KNN with PCA and without tuning the hyper-parameters (with 3 PCs) is = \",\n",
    "      round(metrics.mean_absolute_error(y_test, y_test_pred8),4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we have selected the first 3 PCs, which explain roughly a 73% of the deviance of the data, we observe a rise of the MAE with respect to using all PCs. This is expected, we are training the model with less information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Third pipeline: feature selection and PCA with KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "imputer = SimpleImputer(strategy = 'mean') # we define the imputer with the fixed method\n",
    "scaler = StandardScaler() # we define the scaler method, necessary for KNN \n",
    "selector = SelectKBest(f_regression) # We define the feature selection method\n",
    "pca = PCA(n_components = 3) # We define the Principal Component Analysis method\n",
    "knn_regression = KNeighborsRegressor() # Finally we define the fitting method\n",
    "\n",
    "combined_pipe = Pipeline([('imputer',imputer),\n",
    "                          ('scaler',scaler),\n",
    "                          ('selector',selector),\n",
    "                          ('pca',pca),\n",
    "                          ('knn_regressor',knn_regression)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_pipe.fit(X_train,y_train)\n",
    "y_test_pred9 = combined_pipe.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MAE for KNN with PCA and FS without tuning the hyper-parameters (with 3 PCs) is =  538.1044\n"
     ]
    }
   ],
   "source": [
    "print(\"The MAE for KNN with PCA and FS without tuning the hyper-parameters (with 3 PCs) is = \",\n",
    "      round(metrics.mean_absolute_error(y_test, y_test_pred9),4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find a very similar MAE that the one we found with only the feature selection method. It is a bit higher since we only consider the first 3 PCs, and then we explain with less information. Again, this result manifests the feature selection needs a parameter tuning.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First hyper-parameter tuning: feature selection and KNN regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we check the parameters that need to be tuned. The imputation method is defined to follow the mean strategy, so we will not tune that parameter. The scaling method does not need any parameter to be tuned. The KBest selection method needs the parameter 'selector__k': 10 to be tuned (in fact, we have observed this is crucial). Finally, the KNN regressor needs to have the number of neighbors tuned ('knn_regressor__n_neighbors'). Let's begin defining the search space for those parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = SimpleImputer(strategy = 'mean') # we define the imputer with the fixed method\n",
    "scaler = StandardScaler() # we define the scaler method, necessary for KNN\n",
    "selector = SelectKBest(f_regression) # We define the feature selection method\n",
    "knn_regression = KNeighborsRegressor() # Finally we define the fitting method\n",
    "\n",
    "fea_sel_pipe = Pipeline([('imputer',imputer),\n",
    "                        ('scaler',scaler),\n",
    "                        ('selector',selector),\n",
    "                        ('knn_regressor',knn_regression)])\n",
    "\n",
    "# This is the initial search space. We will comment bellow how we have used it\n",
    "# param_grid = {'selector__k': range(10,500,2),\n",
    "#              'knn_regressor__n_neighbors': [2,3,4,5,6,7,8,9,10,11,12,13,14]}\n",
    "\n",
    "# This is the second search space.\n",
    "# param_grid = {'selector__k': range(410,500,1),\n",
    "#              'knn_regressor__n_neighbors': [2,3,4,5,6,7,8,9,10,11,12,13,14]}\n",
    "\n",
    "param_grid = {'selector__k': range(440,480,1),\n",
    "              'knn_regressor__n_neighbors': [2,3,4,5,6,7,8,9,10,11,12,13,14]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 10 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    3.5s finished\n"
     ]
    }
   ],
   "source": [
    "fea_sel_pipe = RandomizedSearchCV(fea_sel_pipe,param_grid,\n",
    "                             scoring = 'neg_mean_absolute_error',\n",
    "                             cv = tr_valpart, \n",
    "                             n_jobs = 1, verbose = 1)\n",
    "\n",
    "fea_sel_pipe.fit(X_train , y_train)\n",
    "y_test_pred10 = fea_sel_pipe.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'selector__k': 473, 'knn_regressor__n_neighbors': 10}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fea_sel_pipe.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MAE for KNN with FS and tuning the hyper-parameters is =  322.3671\n"
     ]
    }
   ],
   "source": [
    "print(\"The MAE for KNN with FS and tuning the hyper-parameters is = \",\n",
    "      round(metrics.mean_absolute_error(y_test, y_test_pred10),4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have run this procedure with this search space many times. We find the number of important variables that the method finds is always above 410 and bellow 500. We will redefine the parameter grid search above and left the initial one commented. We also run this new searching space many times. We find the best MAE for values of k between 440 and 480. Therefore, we fix this range for a third and last search space. At this point, we observe a reduction of the MAE for the KNN method given all previous trials for the method. In short, apparently, the feature selection is useful for the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second hyper-parameter tuning: PCA and KNN regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, we need to tune the parameters: 'pca__n_components' and 'knn_regressor__n_neighbors'. They are respectively the number of PCs the model take and the number of neighbors considered in the KNN method. We begin defining the search space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = SimpleImputer(strategy = 'mean') # we define the imputer with the fixed method\n",
    "scaler = StandardScaler() # we define the scaler method, necessary for KNN \n",
    "pca = PCA() # We define the Principal Component Analysis method\n",
    "knn_regression = KNeighborsRegressor() # Finally we define the fitting method\n",
    "\n",
    "PCA_pipe = Pipeline([('imputer',imputer),\n",
    "                         ('scaler',scaler),\n",
    "                        ('pca',pca),\n",
    "                        ('knn_regressor',knn_regression)])\n",
    "\n",
    "param_grid = {'pca__n_components': range(1,24,1),\n",
    "              'knn_regressor__n_neighbors': [2,3,4,5,6,7,8,9,10,11,12,13,14]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 10 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    1.0s finished\n"
     ]
    }
   ],
   "source": [
    "PCA_pipe = RandomizedSearchCV(PCA_pipe,param_grid,\n",
    "                             scoring = 'neg_mean_absolute_error',\n",
    "                             cv = tr_valpart, \n",
    "                             n_jobs = 1, verbose = 1)\n",
    "\n",
    "PCA_pipe = PCA_pipe.fit(X_train , y_train)\n",
    "y_test_pred11 = PCA_pipe.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pca__n_components': 20, 'knn_regressor__n_neighbors': 7}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PCA_pipe.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MAE for KNN with PCA and tuning the hyper-parameters is =  327.6659\n"
     ]
    }
   ],
   "source": [
    "print(\"The MAE for KNN with PCA and tuning the hyper-parameters is = \",\n",
    "      round(metrics.mean_absolute_error(y_test, y_test_pred11),4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note this pipeline is generally tuned for a high value of PCs. This is predictable since the PCA is usually run to save time. When we face a huge data set, with a large number of observations and features, PCA is almost necessary to be able to get a model. However, if we test the best number of PCs in terms of the negative mean absolute error, we will always get the best value as the largest number tested (since it explains the greater part of the variance of the data set). Therefore, we obtain worse results in comparison with the last procedure (just including feature selection)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Third hyper-parameter tuning: feature selection, PCA and KNN regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of the exercise, we need to tune the 'knn_regressor__n_neighbors', the 'pca__n_components' and the 'selector__k'. We are using previous information we have determined in previous sections. For instance, we are using the proper range for the search space of the number of features; or we are considering the 12 first PCs explain 98% of the variance (we consider double of them)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = SimpleImputer(strategy = 'mean') # we define the imputer with the fixed method\n",
    "scaler = StandardScaler() # we define the scaler method, necessary for KNN \n",
    "selector = SelectKBest(f_regression) # We define the feature selection method\n",
    "pca = PCA() # We define the Principal Component Analysis method\n",
    "knn_regression = KNeighborsRegressor() # Finally we define the fitting method\n",
    "\n",
    "combined_pipe = Pipeline([('imputer',imputer),\n",
    "                          ('scaler',scaler),\n",
    "                          ('selector',selector),\n",
    "                          ('pca',pca),\n",
    "                          ('knn_regressor',knn_regression)])\n",
    "\n",
    "param_grid = {'selector__k': range(440,480,1),\n",
    "              'pca__n_components': range(1,24,1),\n",
    "              'knn_regressor__n_neighbors': [2,3,4,5,6,7,8,9,10,11,12,13,14]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 10 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    0.8s finished\n"
     ]
    }
   ],
   "source": [
    "combined_pipe = RandomizedSearchCV(combined_pipe,param_grid,\n",
    "                             scoring = 'neg_mean_absolute_error',\n",
    "                             cv = tr_valpart, \n",
    "                             n_jobs = 1, verbose = 1)\n",
    "\n",
    "combined_pipe = combined_pipe.fit(X_train , y_train)\n",
    "y_test_pred12 = combined_pipe.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'selector__k': 474, 'pca__n_components': 21, 'knn_regressor__n_neighbors': 10}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_pipe.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MAE for KNN with FS and PCA and tuning the hyper-parameters is =  321.4497\n"
     ]
    }
   ],
   "source": [
    "print(\"The MAE for KNN with FS and PCA and tuning the hyper-parameters is = \",\n",
    "      round(metrics.mean_absolute_error(y_test, y_test_pred12),4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that systematically we get similar (or a bit above) results than those we obtained in the hyper-parameter tuning section for KNN and just feature selection. This is predictable since we are just including one method of PCA in this pipeline. This method, as commented previously, tends to maximize the deviance explained (maximize the number of PCs). However, in the pipeline with just feaure selection, we included all the relevant information of the data set. In this case, we only include a high percentage of this information, but not all (since we do not consider all PCs). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To conclude, the **best results** in terms of mean absolute error are found for the **first pipeline**: fitting a regression method with KNN after doing feature selection with hyper-parameter tuning. In the previous section, without feature selection, we found results around 330 in the case of no hyper-parameter tuning and around 327 in the case of tuning the number of neighbors considered. In this section, including a feature selection process with the number of attributes tuned, we find a MAE between 322 and 325. Apparently, **there are some attributes that are not relevant** in our data set, and we **find some improvement over the results of the previous sections**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to **get the attributes selected by the first pipeline** (feature selection) tuning the parameters, we are using the best parameters found for the best MAE we have found runing the previous algorithm. Those values are 441 for the selected features and 8 neighbors considered. Under thsese values, we get a MAE equal to 322.34. Considering this, we create a new pipeline, and train it with the training set. Second, we predict the values and verify the MAE is 322.34. Third, we obtain which columns are selected with a boolean set. Fourth, we create an empty list and iterate over the elements of the selected variables (range from 0 to 550 (the total number of attributes). If the attribute is selected (True), the name of the column is saved in the previous empty list. Finally, we **print those names**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The MAE for KNN with PCA and without tuning the hyper-parameters is =  322.3426\n"
     ]
    }
   ],
   "source": [
    "imputer = SimpleImputer(strategy = 'mean') # we define the imputer with the fixed method\n",
    "scaler = StandardScaler() # we define the scaler method, necessary for KNN\n",
    "selector = SelectKBest(f_regression,k = 441) # We define the feature selection method\n",
    "knn_regression = KNeighborsRegressor(n_neighbors = 8) # Finally we define the fitting method\n",
    "\n",
    "fea_sel_pipe = Pipeline([('imputer',imputer),\n",
    "                        ('scaler',scaler),\n",
    "                        ('selector',selector),\n",
    "                        ('knn_regressor',knn_regression)])\n",
    "\n",
    "fea_sel_pipe.fit(X_train,y_train)\n",
    "y_test_pred13 = fea_sel_pipe.predict(X_test)\n",
    "\n",
    "print(\"The MAE for KNN with PCA and without tuning the hyper-parameters is = \",\n",
    "      round(metrics.mean_absolute_error(y_test, y_test_pred13),4))\n",
    "\n",
    "selected = fea_sel_pipe['selector'].get_support() # Getting the boolean set of columns selected.\n",
    "\n",
    "selected_col = list()\n",
    "for i in range(len(selected)):\n",
    "    if(selected[i] == True):\n",
    "        selected_col.append(list(X_train)[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The names of the selected columns are:  ['p54.162.1', 'p54.162.2', 'p54.162.3', 'p54.162.4', 'p54.162.5', 'p54.162.6', 'p54.162.7', 'p54.162.8', 'p54.162.9', 'p54.162.10', 'p54.162.11', 'p54.162.12', 'p54.162.13', 'p54.162.14', 'p54.162.15', 'p54.162.16', 'p54.162.17', 'p54.162.18', 'p54.162.19', 'p54.162.20', 'p54.162.21', 'p54.162.22', 'p54.162.23', 'p54.162.24', 'p54.162.25', 'p59.162.1', 'p59.162.2', 'p59.162.3', 'p59.162.4', 'p59.162.5', 'p59.162.6', 'p59.162.7', 'p59.162.8', 'p59.162.9', 'p59.162.10', 'p59.162.11', 'p59.162.12', 'p59.162.13', 'p59.162.14', 'p59.162.15', 'p59.162.16', 'p59.162.17', 'p59.162.18', 'p59.162.19', 'p59.162.20', 'p59.162.21', 'p59.162.22', 'p59.162.23', 'p59.162.24', 'p59.162.25', 'lai_lv.1', 'lai_lv.2', 'lai_lv.3', 'lai_lv.4', 'lai_lv.5', 'lai_lv.6', 'lai_lv.7', 'lai_lv.8', 'lai_lv.9', 'lai_lv.10', 'lai_lv.11', 'lai_lv.12', 'lai_lv.13', 'lai_lv.14', 'lai_lv.15', 'lai_lv.16', 'lai_lv.17', 'lai_lv.18', 'lai_lv.19', 'lai_lv.20', 'lai_lv.21', 'lai_lv.22', 'lai_lv.23', 'lai_lv.24', 'lai_lv.25', 'lai_hv.1', 'lai_hv.2', 'lai_hv.3', 'lai_hv.4', 'lai_hv.5', 'lai_hv.6', 'lai_hv.7', 'lai_hv.8', 'lai_hv.9', 'lai_hv.10', 'lai_hv.11', 'lai_hv.12', 'lai_hv.13', 'lai_hv.14', 'lai_hv.15', 'lai_hv.16', 'lai_hv.17', 'lai_hv.18', 'lai_hv.19', 'lai_hv.20', 'lai_hv.21', 'lai_hv.22', 'lai_hv.23', 'lai_hv.24', 'lai_hv.25', 'u10n.1', 'u10n.2', 'u10n.3', 'u10n.4', 'u10n.5', 'u10n.6', 'u10n.7', 'u10n.8', 'u10n.9', 'u10n.10', 'u10n.11', 'u10n.12', 'u10n.13', 'u10n.14', 'u10n.15', 'u10n.16', 'u10n.17', 'u10n.18', 'u10n.19', 'u10n.20', 'u10n.21', 'u10n.22', 'u10n.23', 'u10n.24', 'u10n.25', 'v10n.1', 'v10n.2', 'v10n.3', 'v10n.4', 'v10n.5', 'v10n.6', 'v10n.7', 'v10n.8', 'v10n.9', 'v10n.10', 'v10n.11', 'v10n.12', 'v10n.13', 'v10n.14', 'v10n.15', 'v10n.16', 'v10n.17', 'v10n.18', 'v10n.19', 'v10n.20', 'v10n.21', 'v10n.22', 'v10n.23', 'v10n.24', 'v10n.25', 'sp.1', 'sp.2', 'sp.3', 'sp.4', 'sp.5', 'sp.6', 'sp.7', 'sp.8', 'sp.9', 'sp.10', 'sp.11', 'sp.12', 'sp.13', 'sp.14', 'sp.15', 'sp.16', 'sp.17', 'sp.18', 'sp.19', 'sp.20', 'sp.21', 'sp.22', 'sp.23', 'sp.24', 'sp.25', 'stl1.1', 'stl1.2', 'stl1.3', 'stl1.4', 'stl1.5', 'stl1.6', 'stl1.7', 'stl1.8', 'stl1.9', 'stl1.10', 'stl1.11', 'stl1.12', 'stl1.13', 'stl1.14', 'stl1.15', 'stl1.16', 'stl1.17', 'stl1.18', 'stl1.19', 'stl1.20', 'stl1.21', 'stl1.22', 'stl1.23', 'stl1.24', 'stl1.25', 'u10.1', 'u10.2', 'u10.3', 'u10.4', 'u10.5', 'u10.6', 'u10.7', 'u10.8', 'u10.9', 'u10.10', 'u10.11', 'u10.12', 'u10.13', 'u10.14', 'u10.15', 'u10.16', 'u10.17', 'u10.18', 'u10.19', 'u10.20', 'u10.21', 'u10.22', 'u10.23', 'u10.24', 'u10.25', 'v10.1', 'v10.2', 'v10.3', 'v10.4', 'v10.5', 'v10.6', 'v10.7', 'v10.8', 'v10.9', 'v10.10', 'v10.11', 'v10.12', 'v10.13', 'v10.14', 'v10.15', 'v10.16', 'v10.17', 'v10.18', 'v10.19', 'v10.20', 'v10.21', 'v10.22', 'v10.23', 'v10.24', 'v10.25', 't2m.4', 't2m.5', 't2m.10', 't2m.15', 't2m.20', 't2m.24', 't2m.25', 'stl2.1', 'stl2.2', 'stl2.3', 'stl2.4', 'stl2.5', 'stl2.6', 'stl2.7', 'stl2.8', 'stl2.9', 'stl2.10', 'stl2.11', 'stl2.12', 'stl2.13', 'stl2.14', 'stl2.15', 'stl2.16', 'stl2.17', 'stl2.18', 'stl2.19', 'stl2.20', 'stl2.21', 'stl2.22', 'stl2.23', 'stl2.24', 'stl2.25', 'stl3.1', 'stl3.2', 'stl3.3', 'stl3.4', 'stl3.5', 'stl3.6', 'stl3.7', 'stl3.8', 'stl3.9', 'stl3.10', 'stl3.11', 'stl3.12', 'stl3.13', 'stl3.14', 'stl3.15', 'stl3.17', 'stl3.18', 'stl3.19', 'stl3.20', 'stl3.21', 'stl3.22', 'stl3.23', 'stl3.24', 'stl3.25', 'iews.1', 'iews.2', 'iews.3', 'iews.4', 'iews.5', 'iews.6', 'iews.7', 'iews.8', 'iews.9', 'iews.10', 'iews.11', 'iews.12', 'iews.13', 'iews.14', 'iews.15', 'iews.16', 'iews.17', 'iews.18', 'iews.19', 'iews.20', 'iews.21', 'iews.22', 'iews.23', 'iews.24', 'iews.25', 'inss.1', 'inss.2', 'inss.3', 'inss.4', 'inss.5', 'inss.6', 'inss.7', 'inss.8', 'inss.9', 'inss.10', 'inss.11', 'inss.12', 'inss.13', 'inss.14', 'inss.15', 'inss.16', 'inss.17', 'inss.18', 'inss.19', 'inss.20', 'inss.21', 'inss.22', 'inss.23', 'inss.24', 'inss.25', 'fsr.1', 'fsr.2', 'fsr.3', 'fsr.4', 'fsr.5', 'fsr.6', 'fsr.7', 'fsr.8', 'fsr.9', 'fsr.10', 'fsr.11', 'fsr.12', 'fsr.13', 'fsr.14', 'fsr.15', 'fsr.16', 'fsr.17', 'fsr.18', 'fsr.19', 'fsr.20', 'fsr.21', 'fsr.22', 'fsr.23', 'fsr.24', 'fsr.25', 'flsr.16', 'flsr.17', 'flsr.18', 'flsr.19', 'flsr.20', 'flsr.21', 'flsr.22', 'flsr.23', 'flsr.24', 'flsr.25', 'u100.1', 'u100.2', 'u100.3', 'u100.4', 'u100.5', 'u100.6', 'u100.7', 'u100.8', 'u100.9', 'u100.10', 'u100.11', 'u100.12', 'u100.13', 'u100.14', 'u100.15', 'u100.16', 'u100.17', 'u100.18', 'u100.19', 'u100.20', 'u100.21', 'u100.22', 'u100.23', 'u100.24', 'u100.25', 'v100.1', 'v100.2', 'v100.3', 'v100.4', 'v100.5', 'v100.6', 'v100.7', 'v100.8', 'v100.9', 'v100.10', 'v100.11', 'v100.12', 'v100.13', 'v100.14', 'v100.15', 'v100.16', 'v100.17', 'v100.18', 'v100.19', 'v100.20', 'v100.21', 'v100.22', 'v100.23', 'v100.24', 'v100.25']\n"
     ]
    }
   ],
   "source": [
    "print(\"The names of the selected columns are: \",selected_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This previous list of selected columns consists of 441. We are going to check how many of these elements correspond to each location of the 25 existing in our database. In this way we will know if **the method tend to select attributes which belong to the Sotavento** or not. It should be noted that the Sotavento location corresponds to number 13.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17, 17, 17, 18, 18, 17, 17, 17, 17, 18, 17, 17, 17, 17, 18, 17, 18, 18, 18, 19, 18, 18, 18, 19, 19]\n"
     ]
    }
   ],
   "source": [
    "number = [t[len(t)-2:len(t)] for t in selected_col]\n",
    "iter = [\".1\",\".2\",\".3\",\".4\",\".5\",\".6\",\".7\",\".8\",\".9\",10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25]\n",
    "print([len([a for a in number if str(j) in a]) for j in iter])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the number of columns that belong to each location **does not vary between some locations and others**. All values are between 17-19, that is, they are uniformly selected. Therefore, the location that interests us, Sotavento, cannot be said to be more relevant than the others.\n",
    "\n",
    "It may happen that 441 is too high a value for the feature selection, taking into account that the maximum value is 550. This high value may cause that no location is more important than the rest, so to finish this study we are going to repeat the same process but in this case forcing the selecting method to take only k = 100 significant variables. Of course, we will lose information and then the MAE will be worse in this case.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SimpleImputer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-52f0715d1d59>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mimputer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSimpleImputer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstrategy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'mean'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# we define the imputer with the fixed method\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mscaler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# we define the scaler method, necessary for KNN\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mselector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSelectKBest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf_regression\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# We define the feature selection method\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mknn_regression\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKNeighborsRegressor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_neighbors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m8\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Finally we define the fitting method\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'SimpleImputer' is not defined"
     ]
    }
   ],
   "source": [
    "imputer = SimpleImputer(strategy = 'mean') # we define the imputer with the fixed method\n",
    "scaler = StandardScaler() # we define the scaler method, necessary for KNN\n",
    "selector = SelectKBest(f_regression,k = 100) # We define the feature selection method\n",
    "knn_regression = KNeighborsRegressor(n_neighbors = 8) # Finally we define the fitting method\n",
    "\n",
    "fea_sel_pipe = Pipeline([('imputer',imputer),\n",
    "                        ('scaler',scaler),\n",
    "                        ('selector',selector),\n",
    "                        ('knn_regressor',knn_regression)])\n",
    "\n",
    "fea_sel_pipe.fit(X_train,y_train)\n",
    "y_test_pred13 = fea_sel_pipe.predict(X_test)\n",
    "\n",
    "print(\"The MAE for KNN with PCA and without tuning the hyper-parameters is = \",\n",
    "      round(metrics.mean_absolute_error(y_test, y_test_pred13),4))\n",
    "\n",
    "selected = fea_sel_pipe['selector'].get_support() # Getting the boolean set of columns selected.\n",
    "\n",
    "selected_col_proof = list()\n",
    "for i in range(len(selected)):\n",
    "    if(selected[i] == True):\n",
    "        selected_col_proof.append(list(X_train)[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The names of the selected columns are:  ['p59.162.1', 'p59.162.2', 'p59.162.3', 'p59.162.4', 'p59.162.5', 'p59.162.6', 'p59.162.7', 'p59.162.8', 'p59.162.9', 'p59.162.10', 'p59.162.11', 'p59.162.12', 'p59.162.13', 'p59.162.14', 'p59.162.15', 'p59.162.16', 'p59.162.17', 'p59.162.18', 'p59.162.19', 'p59.162.20', 'p59.162.21', 'p59.162.22', 'p59.162.23', 'p59.162.24', 'p59.162.25', 'v10n.4', 'v10n.5', 'v10n.8', 'v10n.9', 'v10n.10', 'v10n.13', 'v10n.14', 'v10n.15', 'v10n.18', 'v10n.19', 'v10n.20', 'v10n.23', 'v10n.24', 'v10n.25', 'v10.5', 'v10.9', 'v10.10', 'v10.14', 'v10.15', 'v10.19', 'v10.20', 'v10.24', 'v10.25', 'iews.3', 'iews.4', 'iews.5', 'iews.8', 'iews.9', 'iews.10', 'iews.13', 'iews.14', 'iews.15', 'iews.19', 'iews.20', 'iews.24', 'iews.25', 'inss.1', 'inss.2', 'inss.3', 'inss.4', 'inss.5', 'inss.6', 'inss.7', 'inss.8', 'inss.9', 'inss.10', 'inss.11', 'inss.12', 'inss.13', 'inss.14', 'inss.15', 'inss.16', 'inss.17', 'inss.18', 'inss.19', 'inss.20', 'inss.21', 'inss.22', 'inss.23', 'inss.24', 'inss.25', 'fsr.21', 'flsr.21', 'flsr.22', 'flsr.23', 'flsr.24', 'v100.5', 'v100.9', 'v100.10', 'v100.14', 'v100.15', 'v100.19', 'v100.20', 'v100.24', 'v100.25']\n"
     ]
    }
   ],
   "source": [
    "print(\"The names of the selected columns are: \",selected_col_proof)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 2, 3, 4, 6, 2, 2, 4, 6, 6, 2, 2, 4, 6, 6, 2, 2, 3, 6, 6, 4, 3, 4, 7, 6]\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "number=[t[len(t)-2:len(t)] for t in selected_col_proof]\n",
    "iter=[\".1\",\".2\",\".3\",\".4\",\".5\",\".6\",\".7\",\".8\",\".9\",10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25]\n",
    "list_len=[len([a for a in number if str(j) in a]) for j in iter]\n",
    "print(list_len)\n",
    "print(list_len[12])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After carrying out the same process, we find very similar results. No location is found that stands out with a very high number of appearances. In the case of Sotavento, it appears 4 times, so in general it does not seem that **this location has a special relevance in our method**.\n",
    "\n",
    "Having already the frequency of each location for the two methods estimated for two different k, we are going to check if there is any variable more often preferred. In general, we cannot draw very clear conclusions from these results. But perhaps **locations 24, 25 and 20 have a greater presence in our method**. Since both for k = 441 and k = 100 present higher values than the rest, but these differences are not very significant."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
